"""
- Raw Data
    - Wikipedia articles in HTML format
    - Each equation within a <math> tag. Within is a <semantics> tag. Should have 3 children:
        - <mrow>: display representation, maybe multiple of these per equation?
        - <annotation-xml encoding="MathML-Content">: OPT representation, generated by LaTeXML
        - <annotation encoding="application/x-tex">: LaTeX representation
    - TODO: verify which equations follow the expected format
- Processed Data
    - Each article should have a mix of raw text and formulas
        - The formulas should be surrounded by special characters indicating start and stop
        - The formulas can be stored in separate files, referenced by identifiers in the processed article
        - Each formula should be represented as an OPT
            - Seems like TangentCFT (https://github.com/BehroozMansouri/TangentCFT) can be used for processing these
            - Each symbol will be associated with a type, as well as a position in the tree (level and position)
            - Should be stored in depth-first order
    - Vocabulary
        - Break into operator, variable, and number types
        - Type will indicate to model whether we have a leaf or not, and if we need to use numerical encoding
- Model
    - Based on GPT-2
    - Input
        - For text tokens, use pretrained text embeddings
            - Add learnable text identifier embedding
        - For formula tokens, need to encode: that it's a formula token, its embedding (or numeric encoding), and its position in the tree
    - Output
        - Differentiate between text and formula outputs
"""

import argparse

from pre_process import process_wikipedia_data
from training import pretrain, test_lm
from utils import TrainOptions, initialize_seeds, device

def main():
    if device.type == "cuda":
        print("Running on GPU")
    else:
        print("No GPU found")

    initialize_seeds(221)

    parser = argparse.ArgumentParser("MathGPT")
    # Modes
    parser.add_argument("--preprocess", action="store_true")
    parser.add_argument("--pretrain", action="store_true")
    parser.add_argument("--test_lm", action="store_true")
    # Config
    parser.add_argument("--name")
    parser.add_argument("--batch_size", type=int)
    parser.add_argument("--max_seq_len", type=int)

    args = parser.parse_args()
    arg_dict = {arg: val for arg, val in vars(args).items() if val is not None}

    if args.preprocess:
        process_wikipedia_data()
    if args.pretrain:
        pretrain(args.name, TrainOptions(arg_dict))
    if args.test_lm:
        test_lm(args.name, "data/Blakersâ€“Massey_theorem.json", TrainOptions(arg_dict))

if __name__ == "__main__":
    main()
